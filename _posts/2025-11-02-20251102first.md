---
layout: single
title:  "[2025-11-02] 20251102 개발일지"
categories: devlog
toc: true
---

## 파이썬 크롤링

### 원하는 작업
1. 네이버에 '회사명'을 검색한다
2. 쇼핑 탭에 들어가서 필터링 기능을 사용하여 '쿠팡' 사이트 검색 결과만 살린다
3. 쿠팡 사이트에 하나씩 들어가며 url, 상품번호, 판매자 이름을 따온다
4. url, 상품번호, 판매자 이름을 엑셀에 정리한다
5. 반복

### 대략적인 구현 방법
1. Step 1: Search for '회사명' on Naver and filter by Coupang
```python
import requests
from bs4 import BeautifulSoup
import time

def search_naver_shopping(query, site_filter="coupang.com"):
    """
    Search Naver Shopping for a query and filter by specific site
    """
    # Naver Shopping search URL
    naver_shopping_url = "https://shopping.naver.com/search/all"

    params = {
        'query': query,
        'cat_id': '',
        'frm': 'NVSHTTL'
    }

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    try:
        response = requests.get(naver_shopping_url, params=params, headers=headers, timeout=10)
        response.encoding = 'utf-8'

        if response.status_code == 200:
            print(f"Successfully searched Naver Shopping for '{query}'")
            return response
        else:
            print(f"Error: Status code {response.status_code}")
            return None
    except Exception as e:
        print(f"Error fetching Naver Shopping: {e}")
        return None
```
2. Step 2: Parse search results and extract Coupang links
```python
def extract_coupang_results(html_response, max_results=50):
    """
    Extract Coupang product links from Naver Shopping results
    """
    soup = BeautifulSoup(html_response.content, 'html.parser')
    coupang_results = []

    # Find all product cards (adjust selector based on actual HTML structure)
    products = soup.find_all('div', class_='product')

    for product in products[:max_results]:
        try:
            # Extract product link
            link_elem = product.find('a', class_='product__link')
            if not link_elem or 'coupang' not in link_elem.get('href', '').lower():
                continue

            product_url = link_elem.get('href', '')
            product_name = link_elem.get_text(strip=True)

            coupang_results.append({
                'url': product_url,
                'name': product_name
            })
        except Exception as e:
            print(f"Error parsing product: {e}")
            continue

    return coupang_results
```
3. Step 3: Visit each Coupang product and collect details
```python
def scrape_coupang_product(product_url):
    """
    Scrape product details from Coupang page
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    try:
        response = requests.get(product_url, headers=headers, timeout=10)
        response.encoding = 'utf-8'

        if response.status_code != 200:
            return None

        soup = BeautifulSoup(response.content, 'html.parser')

        # Extract product number (adjust selectors based on actual HTML)
        product_id = extract_product_id_from_url(product_url)

        # Extract seller name
        seller_elem = soup.find('a', class_='seller-name')
        seller_name = seller_elem.get_text(strip=True) if seller_elem else 'Unknown'

        return {
            'url': product_url,
            'product_id': product_id,
            'seller_name': seller_name
        }
    except Exception as e:
        print(f"Error scraping product {product_url}: {e}")
        return None

def extract_product_id_from_url(url):
    """
    Extract product ID from Coupang URL
    Coupang URLs typically have format: ...?itemId=XXXXX
    """
    from urllib.parse import urlparse, parse_qs
    parsed = urlparse(url)
    params = parse_qs(parsed.query)
    return params.get('itemId', ['Unknown'])[0]
```
4. Step 4: Save data to Excel
```python
import openpyxl
from openpyxl.styles import Font, PatternFill

def save_to_excel(data, filename='sellers.xlsx'):
    """
    Save collected data to an Excel file
    """
    workbook = openpyxl.Workbook()
    worksheet = workbook.active
    worksheet.title = 'Sellers'

    # Add headers
    headers = ['URL', 'Product ID', 'Seller Name']
    worksheet.append(headers)

    # Style header row
    header_fill = PatternFill(start_color='CCCCCC', end_color='CCCCCC', fill_type='solid')
    header_font = Font(bold=True)

    for cell in worksheet[1]:
        cell.fill = header_fill
        cell.font = header_font

    # Add data rows
    for item in data:
        worksheet.append([
            item.get('url', ''),
            item.get('product_id', ''),
            item.get('seller_name', '')
        ])

    # Adjust column widths
    worksheet.column_dimensions['A'].width = 50
    worksheet.column_dimensions['B'].width = 15
    worksheet.column_dimensions['C'].width = 30

    workbook.save(filename)
    print(f"Data saved to {filename}")
```
5. Step 5: Main orchestration and repeat logic
```python
def main():
    """
    Main function to orchestrate the scraping process
    """
    query = "Company Name"
    all_results = []

    print("Step 1: Searching Naver Shopping...")
    naver_response = search_naver_shopping(query)

    if not naver_response:
        print("Failed to search Naver Shopping")
        return

    print("Step 2: Extracting Coupang results...")
    coupang_links = extract_coupang_results(naver_response, max_results=50)

    print(f"Found {len(coupang_links)} Coupang products")

    print("Step 3: Scraping product details...")
    for idx, product in enumerate(coupang_links, 1):
        print(f"  Processing {idx}/{len(coupang_links)}: {product['url'][:60]}...")

        details = scrape_coupang_product(product['url'])
        if details:
            all_results.append(details)

        # Be respectful: add delay between requests
        time.sleep(2)

    print(f"Successfully scraped {len(all_results)} products")

    print("Step 4: Saving to Excel...")
    save_to_excel(all_results)

    print("Done!")

if __name__ == "__main__":
    main()
```
